clc
clear
A=[1:1:730]'; %%%%自变量
 
y=[1368355
1323894
962797
1130555
1092372
1214678
1090313
1464080
1107073
1080106
1032268
1333724
1029108
1227326
1201902
1104934
992164
1220442
917362
1554386
1321873
1328181
1249368
1503039
1327081
1254019
1221669
1301155
1334956
1294582
1327686
1277160
1489972
1494176
1415180
1514212
1426286
1452708
1476480
1310006
1085360
247641
710610
762996
923775
1060902
1064214
915027
1049617
1172815
1182266
1111515
961741
1164634
1041551
1061084
913880
1047889
969089
1137890
1060100
1048708
1059944
1158588
1087686
959067
1316700
1171195
933590
892988
1074387
902207
929730
920626
1015975
977392
1140967
1063580
1085114
884016
1061806
1146939
969665
1103784
1016910
894657
1079591
1103484
1067790
1032012
968339
969040
1007526
855115
978961
1060619
1108653
1186165
1107847
1085960
990980
1007304
1042461
1106184
1024980
1155341
1077298
990632
1031302
1200004
1125558
1213682
1089722
1091689
1302472
1167459
1130116
1238768
1043936
1026854
977706
797188
795141
921288
873621
1221602
1273037
1139922
1110670
1441142
1322423
1110340
1103821
1107242
1188532
974491
1275235
1358263
1285164
1201032
1198385
1164287
1211247
1399652
1401924
1204172
1049397
1056830
1091226
914951
1152621
3227901
1896227
1942954
1390479
1462854
1504984
1481407
1704022
1479832
1315930
1248207
1308309
1384440
1227394
1259182
1958000
1616914
3761193
2317801
1395707
1252802
1079640
1072673
1115775
974647
892664
1056951
1112299
1026042
1135301
1134265
1259030
1158339
1187412
1178389
1152417
1177663
1146584
1057406
1096610
1136590
1122385
1335788
1199046
1248646
1194168
1096615
946266
1116175
1096313
1195681
1051036
1116735
1062598
995398
1161706
1199491
1140969
1093093
1162818
1125447
1035422
1172448
1265482
1407734
1316482
1174270
1200461
1216957
1185101
1351900
1223986
1330610
1416709
1235672
1314099
1221039
1263915
1346043
1185611
1222024
1184398
1097237
1333373
1230114
1387160
1317850
1293843
1378791
1355927
1376070
1403389
1599860
1612062
1397526
1418608
1232088
1552992
1354789
1465953
1868514
1379103
1201360
1311621
1486795
1411306
1373773
1555473
1397117
1384531
1331321
1250376
1017025
1274415
1392724
1379396
1258539
1419376
1264863
1461389
1290408
1088978
1112227
939624
950752
853896
874227
1085824
933013
1287330
1214412
1124998
1277091
1145976
1158408
1105602
1275050
1115145
1042163
1261850
1230814
1380373
1288785
1436387
1278454
1162378
1205289
1237494
1244959
1253069
1098179
1213845
1486737
2761451
2406287
2466889
2471618
1887860
1629511
1355999
1903321
1646262
1423027
3585579
2377937
1889339
1402717
1112785
937556
1119777
1151730
1170772
1106342
1041044
1294032
1107580
1113570
949988
1003094
1033132
1072580
1182964
1081318
1167429
1051522
1127721
1120166
1025809
1036449
1134222
1042606
1114346
1284537
1434297
2353105
1454158
1169490
1085136
1153873
989688
1167776
1078865
1082701
1233260
1162530
1187532
1264981
1041759
1100835
1021612
1218274
1245480
1203621
1244323
1252279
1160399
1064995
1248361
1226028
1400736
1329112
1182742
1225772
1902239
1599242
1645779
1471485
1434010
1364632
1537777
1558607
1543990
1727116
1807215
1677413
1886440
1646623
1628991
1724945
1657135
1581955
1515912
1523015
977928
283612
709879
764028
821942
747733
882025
850650
1325645
1149229
1427808
1121718
1413123
1168863
1137528
1302074
1168572
1245242
1313352
1333000
1456541
1203266
1329476
1359040
1334449
1209559
1235829
1336273
1253744
1208107
1420458
1301597
1195621
1427460
1484545
1252443
1283212
1513582
1409574
1230754
1305023
1229603
1234639
1417542
1278570
1368328
1233734
1338458
1269765
1136964
1262077
1301618
1240090
1056143
1051523
1091685
1031888
1312967
1300053
1189237
1093329
1211819
1187603
1036366
1139516
1100734
1317728
1376282
1360974
1496837
1542735
1523775
1564299
1248464
1509372
1505790
1079054
1297789
1591559
1492453
1262837
1360505
1386730
1585220
1919262
2360627
2336195
1930474
1620915
1220984
1176718
1114320
1119377
1234031
1290744
1363521
1531322
1275278
1352344
1520791
1463936
1520447
1522689
1593759
1332583
1216016
1458189
1610760
1384585
1530655
1290531
1459326
1531897
1486621
1392315
1589342
1458544
1332395
1337166
1169255
1224981
1645270
3277859
2140758
1832992
1612173
1435461
1530677
1972485
1670244
1605731
1572108
1630455
1523131
1546527
1365081
1857406
2443266
2295969
3867100
2509206
2101699
1353905
1380847
1507582
1353080
1310115
1352629
1285349
1344851
1273564
1176605
1303384
1290724
1139042
1342721
1284481
1292046
1194002
1226479
1297478
1271599
1319362
1344429
1314822
1364394
1396305
1351625
1197733
1286491
1300385
1386343
1372853
1184114
1167651
1152830
1317174
1347354
1163409
1101413
1178070
1201329
1154240
1203574
1474418
1203003
1318284
1132870
1186234
1075757
1353323
1219311
1236100
1157110
1222010
1128401
1166290
1347872
1497150
1207271
1376315
1412445
1163834
1284598
1439549
1311845
1273988
1314495
1449709
1306241
1318560
1625077
1517523
1394596
1492745
1594645
1442897
1397354
1675519
1707560
1844008
1917184
1645938
1519918
1252383
1330343
1425568
1521550
1371729
1476009
1561906
1229281
1449641
1221410
1406880
1587214
1530122
1438677
1387936
1561444
1472208
1503114
1426205
1382090
1257791
1100493
1147032
1073631
1183682
1148184
1273495
1592523
1425833
1473791
1357376
1363644
1414628
1390417
1253298
1251004
1359506
1499665
1267746
1385441
1668719
1284887
1189904
1491976
1375249
1399768
1479409
1419361
1155195
1130975
1723467
3354844
2492975
2125146
1962225
1585505
1812723
1696122
1888294
1432694
2007521
3437768
2665817
2045278
1700223
1393788
1299323
1488304
1316098
1412265
1354624
1337404
1114866
1077955
1210082
887035
785150
684016
594744
497883
437736
1433991
1104905
1203268
1341268
1486747
1574193
1417968
1280584
1559568
1577863
1742571
1766950
1495670
1390042
1380737
1550366
1514466
1533055
1581044
1592408
1541630
1503484
1484286
1354230
1329876
1441701
1515020
1544938
1400806
1726367
1454644
]'%%%因变量的数据
[n1,n]=size(A) %%%%自变量的个数
[m,m1]=size(y) %%%%因变量的个数
p=A';  %将所有自变量合并得到输入数据矩阵
t=y; %将所有因变量合并目标数据矩阵
%利用premnmx函数对数据进行归一化
 
[pn,minp,maxp,tn,mint,maxt]=premnmx(p,t); % 对于输入矩阵p和输出矩阵t进行归一化处理
u=ones(n,1);
dx=[-1*u,1*u];                   %归一化处理后最小值为-1，最大值为1
%BP网络训练
net=newff(dx,[n,50,m],{'tansig','tansig','purelin'},'traingdx'); %建立模型，并用梯度下降法训练．
 
net.trainParam.show=1000;               %1000轮回显示一次结果
net.trainParam.Lr=0.05;                 %学习速度为0.05
net.trainParam.epochs=50000;           %最大训练轮回为50000次
net.trainParam.goal=0.65*10^(-3);     %均方误差
net=train(net,pn,tn);                   %开始训练，其中pn,tn分别为输入输出样本
%利用原始数据对BP网络仿真
an=sim(net,pn);           %用训练好的模型进行仿真
a=postmnmx(an,mint,maxt); % 把仿真得到的数据还原为原始的数量级；
pnew=[p
 
     ];   %%输入自变量的参数，每一行表示一个自变量，列数表示预测的个数
 
pnewn=tramnmx(pnew,minp,maxp); %利用原始输入数据的归一化参数对新数据进行归一化；
anewn=sim(net,pnewn);            %利用归一化后的数据进行仿真；
anew=postmnmx(anewn,mint,maxt)  %把仿真得到的数据还原为原始的数量级；
 wucha=abs(t-anew)./t   %%%相对误差
 
 
 plot(A,y)
 hold on
 plot(A,anew')
 







 
clc
clear
A=[1:1:730]'; %%%%自变量
 
y=[1368355
1323894
962797
1130555
1092372
1214678
1090313
1464080
1107073
1080106
1032268
1333724
1029108
1227326
1201902
1104934
992164
1220442
917362
1554386
1321873
1328181
1249368
1503039
1327081
1254019
1221669
1301155
1334956
1294582
1327686
1277160
1489972
1494176
1415180
1514212
1426286
1452708
1476480
1310006
1085360
247641
710610
762996
923775
1060902
1064214
915027
1049617
1172815
1182266
1111515
961741
1164634
1041551
1061084
913880
1047889
969089
1137890
1060100
1048708
1059944
1158588
1087686
959067
1316700
1171195
933590
892988
1074387
902207
929730
920626
1015975
977392
1140967
1063580
1085114
884016
1061806
1146939
969665
1103784
1016910
894657
1079591
1103484
1067790
1032012
968339
969040
1007526
855115
978961
1060619
1108653
1186165
1107847
1085960
990980
1007304
1042461
1106184
1024980
1155341
1077298
990632
1031302
1200004
1125558
1213682
1089722
1091689
1302472
1167459
1130116
1238768
1043936
1026854
977706
797188
795141
921288
873621
1221602
1273037
1139922
1110670
1441142
1322423
1110340
1103821
1107242
1188532
974491
1275235
1358263
1285164
1201032
1198385
1164287
1211247
1399652
1401924
1204172
1049397
1056830
1091226
914951
1152621
3227901
1896227
1942954
1390479
1462854
1504984
1481407
1704022
1479832
1315930
1248207
1308309
1384440
1227394
1259182
1958000
1616914
3761193
2317801
1395707
1252802
1079640
1072673
1115775
974647
892664
1056951
1112299
1026042
1135301
1134265
1259030
1158339
1187412
1178389
1152417
1177663
1146584
1057406
1096610
1136590
1122385
1335788
1199046
1248646
1194168
1096615
946266
1116175
1096313
1195681
1051036
1116735
1062598
995398
1161706
1199491
1140969
1093093
1162818
1125447
1035422
1172448
1265482
1407734
1316482
1174270
1200461
1216957
1185101
1351900
1223986
1330610
1416709
1235672
1314099
1221039
1263915
1346043
1185611
1222024
1184398
1097237
1333373
1230114
1387160
1317850
1293843
1378791
1355927
1376070
1403389
1599860
1612062
1397526
1418608
1232088
1552992
1354789
1465953
1868514
1379103
1201360
1311621
1486795
1411306
1373773
1555473
1397117
1384531
1331321
1250376
1017025
1274415
1392724
1379396
1258539
1419376
1264863
1461389
1290408
1088978
1112227
939624
950752
853896
874227
1085824
933013
1287330
1214412
1124998
1277091
1145976
1158408
1105602
1275050
1115145
1042163
1261850
1230814
1380373
1288785
1436387
1278454
1162378
1205289
1237494
1244959
1253069
1098179
1213845
1486737
2761451
2406287
2466889
2471618
1887860
1629511
1355999
1903321
1646262
1423027
3585579
2377937
1889339
1402717
1112785
937556
1119777
1151730
1170772
1106342
1041044
1294032
1107580
1113570
949988
1003094
1033132
1072580
1182964
1081318
1167429
1051522
1127721
1120166
1025809
1036449
1134222
1042606
1114346
1284537
1434297
2353105
1454158
1169490
1085136
1153873
989688
1167776
1078865
1082701
1233260
1162530
1187532
1264981
1041759
1100835
1021612
1218274
1245480
1203621
1244323
1252279
1160399
1064995
1248361
1226028
1400736
1329112
1182742
1225772
1902239
1599242
1645779
1471485
1434010
1364632
1537777
1558607
1543990
1727116
1807215
1677413
1886440
1646623
1628991
1724945
1657135
1581955
1515912
1523015
977928
283612
709879
764028
821942
747733
882025
850650
1325645
1149229
1427808
1121718
1413123
1168863
1137528
1302074
1168572
1245242
1313352
1333000
1456541
1203266
1329476
1359040
1334449
1209559
1235829
1336273
1253744
1208107
1420458
1301597
1195621
1427460
1484545
1252443
1283212
1513582
1409574
1230754
1305023
1229603
1234639
1417542
1278570
1368328
1233734
1338458
1269765
1136964
1262077
1301618
1240090
1056143
1051523
1091685
1031888
1312967
1300053
1189237
1093329
1211819
1187603
1036366
1139516
1100734
1317728
1376282
1360974
1496837
1542735
1523775
1564299
1248464
1509372
1505790
1079054
1297789
1591559
1492453
1262837
1360505
1386730
1585220
1919262
2360627
2336195
1930474
1620915
1220984
1176718
1114320
1119377
1234031
1290744
1363521
1531322
1275278
1352344
1520791
1463936
1520447
1522689
1593759
1332583
1216016
1458189
1610760
1384585
1530655
1290531
1459326
1531897
1486621
1392315
1589342
1458544
1332395
1337166
1169255
1224981
1645270
3277859
2140758
1832992
1612173
1435461
1530677
1972485
1670244
1605731
1572108
1630455
1523131
1546527
1365081
1857406
2443266
2295969
3867100
2509206
2101699
1353905
1380847
1507582
1353080
1310115
1352629
1285349
1344851
1273564
1176605
1303384
1290724
1139042
1342721
1284481
1292046
1194002
1226479
1297478
1271599
1319362
1344429
1314822
1364394
1396305
1351625
1197733
1286491
1300385
1386343
1372853
1184114
1167651
1152830
1317174
1347354
1163409
1101413
1178070
1201329
1154240
1203574
1474418
1203003
1318284
1132870
1186234
1075757
1353323
1219311
1236100
1157110
1222010
1128401
1166290
1347872
1497150
1207271
1376315
1412445
1163834
1284598
1439549
1311845
1273988
1314495
1449709
1306241
1318560
1625077
1517523
1394596
1492745
1594645
1442897
1397354
1675519
1707560
1844008
1917184
1645938
1519918
1252383
1330343
1425568
1521550
1371729
1476009
1561906
1229281
1449641
1221410
1406880
1587214
1530122
1438677
1387936
1561444
1472208
1503114
1426205
1382090
1257791
1100493
1147032
1073631
1183682
1148184
1273495
1592523
1425833
1473791
1357376
1363644
1414628
1390417
1253298
1251004
1359506
1499665
1267746
1385441
1668719
1284887
1189904
1491976
1375249
1399768
1479409
1419361
1155195
1130975
1723467
3354844
2492975
2125146
1962225
1585505
1812723
1696122
1888294
1432694
2007521
3437768
2665817
2045278
1700223
1393788
1299323
1488304
1316098
1412265
1354624
1337404
1114866
1077955
1210082
887035
785150
684016
594744
497883
437736
1433991
1104905
1203268
1341268
1486747
1574193
1417968
1280584
1559568
1577863
1742571
1766950
1495670
1390042
1380737
1550366
1514466
1533055
1581044
1592408
1541630
1503484
1484286
1354230
1329876
1441701
1515020
1544938
1400806
1726367
1454644
]'%%%因变量的数据
[n1,n]=size(A) %%%%自变量的个数
[m,m1]=size(y) %%%%因变量的个数
p=A';  %将所有自变量合并得到输入数据矩阵
t=y; %将所有因变量合并目标数据矩阵
%利用premnmx函数对数据进行归一化
 
[pn,minp,maxp,tn,mint,maxt]=premnmx(p,t); % 对于输入矩阵p和输出矩阵t进行归一化处理
u=ones(n,1);
dx=[-1*u,1*u];                   %归一化处理后最小值为-1，最大值为1
%BP网络训练
net=newff(dx,[n,50,m],{'tansig','tansig','purelin'},'traingdx'); %建立模型，并用梯度下降法训练．
 
net.trainParam.show=1000;               %1000轮回显示一次结果
net.trainParam.Lr=0.05;                 %学习速度为0.05
net.trainParam.epochs=50000;           %最大训练轮回为50000次
net.trainParam.goal=0.65*10^(-3);     %均方误差
net=train(net,pn,tn);                   %开始训练，其中pn,tn分别为输入输出样本
%利用原始数据对BP网络仿真
an=sim(net,pn);           %用训练好的模型进行仿真
a=postmnmx(an,mint,maxt); % 把仿真得到的数据还原为原始的数量级；
pnew=[731:1:761
 
     ];   %%输入自变量的参数，每一行表示一个自变量，列数表示预测的个数
 
pnewn=tramnmx(pnew,minp,maxp); %利用原始输入数据的归一化参数对新数据进行归一化；
anewn=sim(net,pnewn);            %利用归一化后的数据进行仿真；
anew=postmnmx(anewn,mint,maxt)  %把仿真得到的数据还原为原始的数量级；
 
 





function [test_ty]=shengjingwangluo(data,L,N)
price=data'
 
 
 
%% 2.构造样本集
% 数据个数
n=length(price);
 
% 确保price为列向量
price=price(:);
 
% x(n) 由x(n-1),x(n-2),...,x(n-L)共L个数预测得到.
 
 
% price_n：每列为一个构造完毕的样本，共n-L个样本
price_n = zeros(L+1, n-L);
for i=1:n-L
    price_n(:,i) = price(i:i+L);
end
 
%% 划分训练、测试样本
% 将前280份数据划分为训练样本
% 后51份数据划分为测试样本
 
trainx = price_n(1:L-N+1,:);
trainy = price_n(L-N+2:end,:);
[ww,mm]=size(trainx);
testx = price(n-ww+1:n, end);
 
 
%% 创建Elman神经网络
 
% 包含15个神经元，训练函数为traingdx
net=elmannet(1:2,15,'traingdx');
 
% 设置显示级别
net.trainParam.show=1;
 
% 最大迭代次数为2000次
net.trainParam.epochs=2000;
 
% 误差容限，达到此误差就可以停止训练
net.trainParam.goal=0.000001;
 
% 最多验证失败次数
net.trainParam.max_fail=5;
 
% 对网络进行初始化
net=init(net);
 
%% 网络训练
 
%训练数据归一化
[trainx1, st1] = mapminmax(trainx);
[trainy1, st2] = mapminmax(trainy);
 
% 测试数据做与训练数据相同的归一化操作
testx1 = mapminmax('apply',testx,st1);
 
 
% 输入训练样本进行训练
[net,per] = train(net,trainx1,trainy1);
 
%% 测试。输入归一化后的数据，再对实际输出进行反归一化
 
% 将训练数据输入网络进行测试
train_ty1 = sim(net, trainx1);
train_ty = mapminmax('reverse', train_ty1, st2);
 
% 将测试数据输入网络进行测试
test_ty1 = sim(net, testx1);
test_ty = mapminmax('reverse', test_ty1, st2);
  
%% 显示结果
% 1.显示训练数据的测试结果
figure(1)
x=1:length(train_ty);
 
% 显示真实值
plot(x,trainy,'b-');
hold on
% 显示神经网络的输出值
plot(x,train_ty,'r--')
 
legend('货物真实值','Elman网络输出值')
title('训练数据的测试结果');
 
% 显示残差
figure(2)
plot(x, train_ty - trainy)
title('训练数据测试结果的残差')
 
% 显示均方误差
mse1 = mse(train_ty - trainy);
fprintf('    mse = \n     %f\n', mse1)
 
% 显示相对误差
disp('    相对误差：')
fprintf('%f  ', (train_ty - trainy)./trainy );
fprintf('\n')
% 显示预测值
disp('    预测值：')
fprintf('%f  ', test_ty );
fprintf('\n')
end
 


